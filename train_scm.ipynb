{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e09560",
   "metadata": {},
   "source": [
    "# SCM Training\n",
    "Here, we train a small concept model (SCM) for next-concept prediction. We will use the pre-trained PreNet to invert the embeddings into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bbe47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modules.inverter import build_inverter, get_encoder\n",
    "from modules.data import SCMTrainingDataset, get_bookcorpus_for_scm\n",
    "from modules.scm import SmallConceptModel, GenerativeSCM\n",
    "from modules.train import train_scm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d439b3",
   "metadata": {},
   "source": [
    "## Configs\n",
    "We define configs for model and training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d04e1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    \"encoder_id\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "}\n",
    "\n",
    "scm_configs = {\n",
    "    \"d_model\": 512,\n",
    "    \"embed_dim\": 384,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 3,\n",
    "    \"dim_feedforward\": 4 * 512,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_seq_len\": 16,\n",
    "}\n",
    "\n",
    "train_configs = {\n",
    "    \"load_weights\": \"saved_models/scm_v01_multilingual.pth\",\n",
    "    \"save_weights\": \"saved_models/scm_v01_multilingual.pth\",\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"max_target_len\": 64,\n",
    "    \"embed_batch_size\": 128,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"sample_data\": 0.03,\n",
    "    \"num_epochs\": 5,\n",
    "}\n",
    "\n",
    "data_configs = {\n",
    "    \"load_cached\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415798c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca783",
   "metadata": {},
   "source": [
    "## Models\n",
    "We initialize and load the encoder, inverter, and SCM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0069958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(models_config[\"encoder_id\"])\n",
    "inverter = build_inverter()\n",
    "scm = SmallConceptModel(**scm_configs).to(device)\n",
    "\n",
    "if train_configs[\"load_weights\"]:\n",
    "    scm.load_state_dict(torch.load(train_configs[\"load_weights\"], map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0056ce",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We load cached embeddings if available, otherwise we rebuild the dataloader via the `get_bookcorpus_for_scm` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2c9236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [02:51<00:00, 72.98it/s]\n"
     ]
    }
   ],
   "source": [
    "if data_configs[\"load_cached\"]:\n",
    "    embeddings = torch.load(data_configs[\"load_cached\"])\n",
    "    dataset = SCMTrainingDataset(embeddings)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_configs[\"train_batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "else:\n",
    "    dataloader = get_bookcorpus_for_scm(\n",
    "        encoder,\n",
    "        embed_dim=scm_configs[\"embed_dim\"],\n",
    "        train_batch_size=train_configs[\"train_batch_size\"],\n",
    "        embed_batch_size=train_configs[\"embed_batch_size\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540b642",
   "metadata": {},
   "source": [
    "## Training\n",
    "Finally, we can train the model using the `train_sm` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ae9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]  Batch [100/3125]  Loss: 0.065444\n",
      "Epoch [1/5]  Batch [200/3125]  Loss: 0.058237\n",
      "Epoch [1/5]  Batch [300/3125]  Loss: 0.054097\n",
      "Epoch [1/5]  Batch [400/3125]  Loss: 0.049704\n",
      "Epoch [1/5]  Batch [500/3125]  Loss: 0.049940\n",
      "Epoch [1/5]  Batch [600/3125]  Loss: 0.048512\n",
      "Epoch [1/5]  Batch [700/3125]  Loss: 0.050204\n",
      "Epoch [1/5]  Batch [800/3125]  Loss: 0.050097\n",
      "Epoch [1/5]  Batch [900/3125]  Loss: 0.049509\n",
      "Epoch [1/5]  Batch [1000/3125]  Loss: 0.048793\n",
      "Epoch [1/5]  Batch [1100/3125]  Loss: 0.048871\n",
      "Epoch [1/5]  Batch [1200/3125]  Loss: 0.049683\n",
      "Epoch [1/5]  Batch [1300/3125]  Loss: 0.049092\n",
      "Epoch [1/5]  Batch [1400/3125]  Loss: 0.048284\n",
      "Epoch [1/5]  Batch [1500/3125]  Loss: 0.047964\n",
      "Epoch [1/5]  Batch [1600/3125]  Loss: 0.047858\n",
      "Epoch [1/5]  Batch [1700/3125]  Loss: 0.048639\n",
      "Epoch [1/5]  Batch [1800/3125]  Loss: 0.048534\n",
      "Epoch [1/5]  Batch [1900/3125]  Loss: 0.047878\n",
      "Epoch [1/5]  Batch [2000/3125]  Loss: 0.048913\n",
      "Epoch [1/5]  Batch [2100/3125]  Loss: 0.048436\n",
      "Epoch [1/5]  Batch [2200/3125]  Loss: 0.049353\n",
      "Epoch [1/5]  Batch [2300/3125]  Loss: 0.047671\n",
      "Epoch [1/5]  Batch [2400/3125]  Loss: 0.048468\n",
      "Epoch [1/5]  Batch [2500/3125]  Loss: 0.048947\n",
      "Epoch [1/5]  Batch [2600/3125]  Loss: 0.048827\n",
      "Epoch [1/5]  Batch [2700/3125]  Loss: 0.047518\n",
      "Epoch [1/5]  Batch [2800/3125]  Loss: 0.048794\n",
      "Epoch [1/5]  Batch [2900/3125]  Loss: 0.050900\n",
      "Epoch [1/5]  Batch [3000/3125]  Loss: 0.048283\n",
      "Epoch [1/5]  Batch [3100/3125]  Loss: 0.049409\n",
      "*** Epoch 1 Complete.  Avg Loss = 0.068462 ***\n",
      "Epoch [2/5]  Batch [100/3125]  Loss: 0.048725\n",
      "Epoch [2/5]  Batch [200/3125]  Loss: 0.047982\n",
      "Epoch [2/5]  Batch [300/3125]  Loss: 0.048486\n",
      "Epoch [2/5]  Batch [400/3125]  Loss: 0.048514\n",
      "Epoch [2/5]  Batch [500/3125]  Loss: 0.047316\n",
      "Epoch [2/5]  Batch [600/3125]  Loss: 0.048721\n",
      "Epoch [2/5]  Batch [700/3125]  Loss: 0.047643\n",
      "Epoch [2/5]  Batch [800/3125]  Loss: 0.047770\n",
      "Epoch [2/5]  Batch [900/3125]  Loss: 0.049997\n",
      "Epoch [2/5]  Batch [1000/3125]  Loss: 0.048893\n",
      "Epoch [2/5]  Batch [1100/3125]  Loss: 0.048298\n",
      "Epoch [2/5]  Batch [1200/3125]  Loss: 0.049135\n",
      "Epoch [2/5]  Batch [1300/3125]  Loss: 0.048480\n",
      "Epoch [2/5]  Batch [1400/3125]  Loss: 0.047206\n",
      "Epoch [2/5]  Batch [1500/3125]  Loss: 0.048899\n",
      "Epoch [2/5]  Batch [1600/3125]  Loss: 0.048920\n",
      "Epoch [2/5]  Batch [1700/3125]  Loss: 0.047891\n",
      "Epoch [2/5]  Batch [1800/3125]  Loss: 0.048859\n",
      "Epoch [2/5]  Batch [1900/3125]  Loss: 0.047338\n",
      "Epoch [2/5]  Batch [2000/3125]  Loss: 0.049197\n",
      "Epoch [2/5]  Batch [2100/3125]  Loss: 0.047109\n",
      "Epoch [2/5]  Batch [2200/3125]  Loss: 0.049442\n",
      "Epoch [2/5]  Batch [2300/3125]  Loss: 0.047986\n",
      "Epoch [2/5]  Batch [2400/3125]  Loss: 0.049853\n",
      "Epoch [2/5]  Batch [2500/3125]  Loss: 0.048484\n",
      "Epoch [2/5]  Batch [2600/3125]  Loss: 0.046758\n",
      "Epoch [2/5]  Batch [2700/3125]  Loss: 0.048249\n",
      "Epoch [2/5]  Batch [2800/3125]  Loss: 0.048790\n",
      "Epoch [2/5]  Batch [2900/3125]  Loss: 0.048546\n",
      "Epoch [2/5]  Batch [3000/3125]  Loss: 0.047918\n",
      "Epoch [2/5]  Batch [3100/3125]  Loss: 0.049368\n",
      "*** Epoch 2 Complete.  Avg Loss = 0.048337 ***\n",
      "Epoch [3/5]  Batch [100/3125]  Loss: 0.048043\n",
      "Epoch [3/5]  Batch [200/3125]  Loss: 0.048112\n",
      "Epoch [3/5]  Batch [300/3125]  Loss: 0.048145\n",
      "Epoch [3/5]  Batch [400/3125]  Loss: 0.048197\n",
      "Epoch [3/5]  Batch [500/3125]  Loss: 0.048336\n",
      "Epoch [3/5]  Batch [600/3125]  Loss: 0.047972\n",
      "Epoch [3/5]  Batch [700/3125]  Loss: 0.046315\n",
      "Epoch [3/5]  Batch [800/3125]  Loss: 0.047483\n",
      "Epoch [3/5]  Batch [900/3125]  Loss: 0.047472\n",
      "Epoch [3/5]  Batch [1000/3125]  Loss: 0.048057\n",
      "Epoch [3/5]  Batch [1100/3125]  Loss: 0.048575\n",
      "Epoch [3/5]  Batch [1200/3125]  Loss: 0.048396\n",
      "Epoch [3/5]  Batch [1300/3125]  Loss: 0.048498\n",
      "Epoch [3/5]  Batch [1400/3125]  Loss: 0.048931\n",
      "Epoch [3/5]  Batch [1500/3125]  Loss: 0.048964\n",
      "Epoch [3/5]  Batch [1600/3125]  Loss: 0.048252\n",
      "Epoch [3/5]  Batch [1700/3125]  Loss: 0.048329\n",
      "Epoch [3/5]  Batch [1800/3125]  Loss: 0.048278\n",
      "Epoch [3/5]  Batch [1900/3125]  Loss: 0.047317\n",
      "Epoch [3/5]  Batch [2000/3125]  Loss: 0.048534\n",
      "Epoch [3/5]  Batch [2100/3125]  Loss: 0.047472\n",
      "Epoch [3/5]  Batch [2200/3125]  Loss: 0.049166\n",
      "Epoch [3/5]  Batch [2300/3125]  Loss: 0.048135\n",
      "Epoch [3/5]  Batch [2400/3125]  Loss: 0.048165\n",
      "Epoch [3/5]  Batch [2500/3125]  Loss: 0.048610\n",
      "Epoch [3/5]  Batch [2600/3125]  Loss: 0.049128\n",
      "Epoch [3/5]  Batch [2700/3125]  Loss: 0.049074\n",
      "Epoch [3/5]  Batch [2800/3125]  Loss: 0.048207\n",
      "Epoch [3/5]  Batch [2900/3125]  Loss: 0.049325\n",
      "Epoch [3/5]  Batch [3000/3125]  Loss: 0.049971\n",
      "Epoch [3/5]  Batch [3100/3125]  Loss: 0.047905\n",
      "*** Epoch 3 Complete.  Avg Loss = 0.048349 ***\n",
      "Epoch [4/5]  Batch [100/3125]  Loss: 0.047976\n",
      "Epoch [4/5]  Batch [200/3125]  Loss: 0.048313\n",
      "Epoch [4/5]  Batch [300/3125]  Loss: 0.049055\n",
      "Epoch [4/5]  Batch [400/3125]  Loss: 0.049311\n",
      "Epoch [4/5]  Batch [500/3125]  Loss: 0.048603\n",
      "Epoch [4/5]  Batch [600/3125]  Loss: 0.048012\n",
      "Epoch [4/5]  Batch [700/3125]  Loss: 0.049177\n",
      "Epoch [4/5]  Batch [800/3125]  Loss: 0.048873\n",
      "Epoch [4/5]  Batch [900/3125]  Loss: 0.047717\n",
      "Epoch [4/5]  Batch [1000/3125]  Loss: 0.047841\n",
      "Epoch [4/5]  Batch [1100/3125]  Loss: 0.048771\n",
      "Epoch [4/5]  Batch [1200/3125]  Loss: 0.048409\n",
      "Epoch [4/5]  Batch [1300/3125]  Loss: 0.048474\n",
      "Epoch [4/5]  Batch [1400/3125]  Loss: 0.048190\n",
      "Epoch [4/5]  Batch [1500/3125]  Loss: 0.048485\n",
      "Epoch [4/5]  Batch [1600/3125]  Loss: 0.047448\n",
      "Epoch [4/5]  Batch [1700/3125]  Loss: 0.048459\n",
      "Epoch [4/5]  Batch [1800/3125]  Loss: 0.047898\n",
      "Epoch [4/5]  Batch [1900/3125]  Loss: 0.050452\n",
      "Epoch [4/5]  Batch [2000/3125]  Loss: 0.048481\n",
      "Epoch [4/5]  Batch [2100/3125]  Loss: 0.049735\n",
      "Epoch [4/5]  Batch [2200/3125]  Loss: 0.048705\n",
      "Epoch [4/5]  Batch [2300/3125]  Loss: 0.048826\n",
      "Epoch [4/5]  Batch [2400/3125]  Loss: 0.048989\n",
      "Epoch [4/5]  Batch [2500/3125]  Loss: 0.046956\n",
      "Epoch [4/5]  Batch [2600/3125]  Loss: 0.049450\n",
      "Epoch [4/5]  Batch [2700/3125]  Loss: 0.048997\n",
      "Epoch [4/5]  Batch [2800/3125]  Loss: 0.048311\n",
      "Epoch [4/5]  Batch [2900/3125]  Loss: 0.049472\n",
      "Epoch [4/5]  Batch [3000/3125]  Loss: 0.048088\n",
      "Epoch [4/5]  Batch [3100/3125]  Loss: 0.047981\n",
      "*** Epoch 4 Complete.  Avg Loss = 0.048385 ***\n",
      "Epoch [5/5]  Batch [100/3125]  Loss: 0.048322\n",
      "Epoch [5/5]  Batch [200/3125]  Loss: 0.049211\n",
      "Epoch [5/5]  Batch [300/3125]  Loss: 0.048553\n",
      "Epoch [5/5]  Batch [400/3125]  Loss: 0.047334\n",
      "Epoch [5/5]  Batch [500/3125]  Loss: 0.047918\n",
      "Epoch [5/5]  Batch [600/3125]  Loss: 0.047031\n",
      "Epoch [5/5]  Batch [700/3125]  Loss: 0.048134\n",
      "Epoch [5/5]  Batch [800/3125]  Loss: 0.049441\n",
      "Epoch [5/5]  Batch [900/3125]  Loss: 0.048222\n",
      "Epoch [5/5]  Batch [1000/3125]  Loss: 0.048952\n",
      "Epoch [5/5]  Batch [1100/3125]  Loss: 0.047336\n",
      "Epoch [5/5]  Batch [1200/3125]  Loss: 0.047543\n",
      "Epoch [5/5]  Batch [1300/3125]  Loss: 0.049062\n",
      "Epoch [5/5]  Batch [1400/3125]  Loss: 0.048662\n",
      "Epoch [5/5]  Batch [1500/3125]  Loss: 0.048353\n",
      "Epoch [5/5]  Batch [1600/3125]  Loss: 0.048063\n",
      "Epoch [5/5]  Batch [1700/3125]  Loss: 0.047993\n",
      "Epoch [5/5]  Batch [1800/3125]  Loss: 0.048320\n",
      "Epoch [5/5]  Batch [1900/3125]  Loss: 0.048376\n",
      "Epoch [5/5]  Batch [2000/3125]  Loss: 0.048603\n",
      "Epoch [5/5]  Batch [2100/3125]  Loss: 0.049381\n",
      "Epoch [5/5]  Batch [2200/3125]  Loss: 0.047786\n",
      "Epoch [5/5]  Batch [2300/3125]  Loss: 0.047447\n",
      "Epoch [5/5]  Batch [2400/3125]  Loss: 0.049696\n",
      "Epoch [5/5]  Batch [2500/3125]  Loss: 0.048100\n",
      "Epoch [5/5]  Batch [2600/3125]  Loss: 0.047752\n",
      "Epoch [5/5]  Batch [2700/3125]  Loss: 0.049336\n",
      "Epoch [5/5]  Batch [2800/3125]  Loss: 0.048748\n",
      "Epoch [5/5]  Batch [2900/3125]  Loss: 0.048096\n",
      "Epoch [5/5]  Batch [3000/3125]  Loss: 0.047422\n",
      "Epoch [5/5]  Batch [3100/3125]  Loss: 0.049681\n",
      "*** Epoch 5 Complete.  Avg Loss = 0.048426 ***\n"
     ]
    }
   ],
   "source": [
    "train_scm(\n",
    "    scm,\n",
    "    dataloader,\n",
    "    scm_configs[\"max_seq_len\"],\n",
    "    train_configs[\"lr\"],\n",
    "    train_configs[\"weight_decay\"],\n",
    "    train_configs[\"num_epochs\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474f5e5",
   "metadata": {},
   "source": [
    "Save the updated weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a4669de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scm.state_dict(), train_configs[\"save_weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec2df2",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We can test the model at inference time using the `GenerativeSCM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2768dd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" this is a research project the university has initiated . '' said this prospective study , this is an evaluation that involves more than one theory . '' that is\",\n",
       " \" it 's neural networks . '' zebra 's rodents are similar to zebra 's neural networks . '' does gerald 's lab scientist\",\n",
       " \" they 've been studying these interactions since the beginning . ''\\n\\n'' and eka 've been talking about the concept of divine modality ,\",\n",
       " \" who is the programmer ? '' delaney replied , hoping that the information she gathered will help him defeat the accursed algorithm . '' that is , until\",\n",
       " ' the reason why it seems like every single other organism in the parallel universe has been studying the process , despite having something to hide , is that it means',\n",
       " \" '' it is possible to explain this entire process by comparing the living universe to an electronic system , or to a black hole . '' the professor says ,\",\n",
       " \" we , as a nation , are aware of how the geometric systems of science are making the world more and more difficult for our people to understand . ''\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_scm = GenerativeSCM(scm, encoder, inverter)\n",
    "\n",
    "sentences = [\n",
    "    \"questa è una proposta di ricerca universitaria .\",\n",
    "    \"parla di reti neurali .\"\n",
    "]\n",
    "\n",
    "gen_scm.generate(sentences, sigma_noise=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
