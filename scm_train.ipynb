{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0369f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a837f3",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94575842",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH     = \"saved_models/train_seq_embeddings.pt\"       # path to your (100k, 16, 384) NumPy file\n",
    "BATCH_SIZE    = 32\n",
    "NUM_EPOCHS    = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "SEQ_LEN       = 16               # total length of each sequence \n",
    "EMBED_DIM     = 384              # dimension of each sentence embedding\n",
    "D_MODEL       = 512              # model dimension (we keep it = EMBED_DIM)\n",
    "NUM_LAYERS    = 3                # number of Transformer layers\n",
    "NUM_HEADS     = 4                # number of attention heads\n",
    "FFN_DIM       = 4 * D_MODEL      # feed‐forward “intermediate” dimension\n",
    "DROPOUT       = 0.1\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48072d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106ea7d",
   "metadata": {},
   "source": [
    "## Data\n",
    "First, load the cached embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f567c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf90204",
   "metadata": {},
   "source": [
    "Then, we define a custom sentence vector dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc70b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbeddingDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor):\n",
    "        assert embeddings.ndim == 3\n",
    "        assert embeddings.shape[1] == SEQ_LEN\n",
    "        assert embeddings.shape[2] == EMBED_DIM\n",
    "        \n",
    "        self.data = data.float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        input_seq  = seq[: SEQ_LEN - 1, :]  # (15, 384)\n",
    "        target_seq = seq[1: SEQ_LEN, :]     # (15, 384)\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879cfea",
   "metadata": {},
   "source": [
    "Now create the dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentenceEmbeddingDataset(embeddings)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d5f60",
   "metadata": {},
   "source": [
    "Also, we need to generate the causal mask we will use to mask future vectors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "202d50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "    return torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "\n",
    "causal_mask = generate_causal_mask(SEQ_LEN - 1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b5459",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98306e1d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21f7184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerNextEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 num_layers: int,\n",
    "                 dim_feedforward: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(EMBED_DIM, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.output_projection = nn.Linear(d_model, EMBED_DIM)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_projection(src)\n",
    "        x_proj = self.pos_encoder(x) * math.sqrt(self.d_model)\n",
    "        float_mask = torch.zeros_like(src_mask, dtype=torch.float32)\n",
    "        float_mask = float_mask.masked_fill(src_mask, float(\"-1e9\"))\n",
    "        enc_output = self.transformer_encoder(x_proj, mask=float_mask)\n",
    "        return self.output_projection(enc_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359a7ee",
   "metadata": {},
   "source": [
    "Initialize the model and define the optimizer and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efd5d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 9851264\n"
     ]
    }
   ],
   "source": [
    "model = TransformerNextEmbedding(\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dim_feedforward=FFN_DIM,\n",
    "        dropout=DROPOUT,\n",
    "        max_seq_len=SEQ_LEN\n",
    "    ).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
    "criterion = nn.MSELoss(reduction=\"none\") # we’ll mask the last position manually\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b5421",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf47dbc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81d04501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]  Batch [100/3125]  Loss: 0.010688\n",
      "Epoch [1/5]  Batch [200/3125]  Loss: 0.008928\n",
      "Epoch [1/5]  Batch [300/3125]  Loss: 0.007678\n",
      "Epoch [1/5]  Batch [400/3125]  Loss: 0.006708\n",
      "Epoch [1/5]  Batch [500/3125]  Loss: 0.005631\n",
      "Epoch [1/5]  Batch [600/3125]  Loss: 0.004518\n",
      "Epoch [1/5]  Batch [700/3125]  Loss: 0.003671\n",
      "Epoch [1/5]  Batch [800/3125]  Loss: 0.003250\n",
      "Epoch [1/5]  Batch [900/3125]  Loss: 0.003031\n",
      "Epoch [1/5]  Batch [1000/3125]  Loss: 0.002799\n",
      "Epoch [1/5]  Batch [1100/3125]  Loss: 0.002704\n",
      "Epoch [1/5]  Batch [1200/3125]  Loss: 0.002606\n",
      "Epoch [1/5]  Batch [1300/3125]  Loss: 0.002565\n",
      "Epoch [1/5]  Batch [1400/3125]  Loss: 0.002524\n",
      "Epoch [1/5]  Batch [1500/3125]  Loss: 0.002459\n",
      "Epoch [1/5]  Batch [1600/3125]  Loss: 0.002450\n",
      "Epoch [1/5]  Batch [1700/3125]  Loss: 0.002464\n",
      "Epoch [1/5]  Batch [1800/3125]  Loss: 0.002382\n",
      "Epoch [1/5]  Batch [1900/3125]  Loss: 0.002398\n",
      "Epoch [1/5]  Batch [2000/3125]  Loss: 0.002381\n",
      "Epoch [1/5]  Batch [2100/3125]  Loss: 0.002385\n",
      "Epoch [1/5]  Batch [2200/3125]  Loss: 0.002363\n",
      "Epoch [1/5]  Batch [2300/3125]  Loss: 0.002380\n",
      "Epoch [1/5]  Batch [2400/3125]  Loss: 0.002350\n",
      "Epoch [1/5]  Batch [2500/3125]  Loss: 0.002336\n",
      "Epoch [1/5]  Batch [2600/3125]  Loss: 0.002330\n",
      "Epoch [1/5]  Batch [2700/3125]  Loss: 0.002329\n",
      "Epoch [1/5]  Batch [2800/3125]  Loss: 0.002323\n",
      "Epoch [1/5]  Batch [2900/3125]  Loss: 0.002302\n",
      "Epoch [1/5]  Batch [3000/3125]  Loss: 0.002304\n",
      "Epoch [1/5]  Batch [3100/3125]  Loss: 0.002316\n",
      "*** Epoch 1 Complete.  Avg Loss = 0.003981 ***\n",
      "Epoch [2/5]  Batch [100/3125]  Loss: 0.002288\n",
      "Epoch [2/5]  Batch [200/3125]  Loss: 0.002305\n",
      "Epoch [2/5]  Batch [300/3125]  Loss: 0.002285\n",
      "Epoch [2/5]  Batch [400/3125]  Loss: 0.002297\n",
      "Epoch [2/5]  Batch [500/3125]  Loss: 0.002289\n",
      "Epoch [2/5]  Batch [600/3125]  Loss: 0.002263\n",
      "Epoch [2/5]  Batch [700/3125]  Loss: 0.002295\n",
      "Epoch [2/5]  Batch [800/3125]  Loss: 0.002239\n",
      "Epoch [2/5]  Batch [900/3125]  Loss: 0.002303\n",
      "Epoch [2/5]  Batch [1000/3125]  Loss: 0.002246\n",
      "Epoch [2/5]  Batch [1100/3125]  Loss: 0.002238\n",
      "Epoch [2/5]  Batch [1200/3125]  Loss: 0.002239\n",
      "Epoch [2/5]  Batch [1300/3125]  Loss: 0.002269\n",
      "Epoch [2/5]  Batch [1400/3125]  Loss: 0.002269\n",
      "Epoch [2/5]  Batch [1500/3125]  Loss: 0.002243\n",
      "Epoch [2/5]  Batch [1600/3125]  Loss: 0.002241\n",
      "Epoch [2/5]  Batch [1700/3125]  Loss: 0.002263\n",
      "Epoch [2/5]  Batch [1800/3125]  Loss: 0.002209\n",
      "Epoch [2/5]  Batch [1900/3125]  Loss: 0.002243\n",
      "Epoch [2/5]  Batch [2000/3125]  Loss: 0.002236\n",
      "Epoch [2/5]  Batch [2100/3125]  Loss: 0.002254\n",
      "Epoch [2/5]  Batch [2200/3125]  Loss: 0.002239\n",
      "Epoch [2/5]  Batch [2300/3125]  Loss: 0.002257\n",
      "Epoch [2/5]  Batch [2400/3125]  Loss: 0.002242\n",
      "Epoch [2/5]  Batch [2500/3125]  Loss: 0.002236\n",
      "Epoch [2/5]  Batch [2600/3125]  Loss: 0.002216\n",
      "Epoch [2/5]  Batch [2700/3125]  Loss: 0.002226\n",
      "Epoch [2/5]  Batch [2800/3125]  Loss: 0.002245\n",
      "Epoch [2/5]  Batch [2900/3125]  Loss: 0.002233\n",
      "Epoch [2/5]  Batch [3000/3125]  Loss: 0.002222\n",
      "Epoch [2/5]  Batch [3100/3125]  Loss: 0.002253\n",
      "*** Epoch 2 Complete.  Avg Loss = 0.002252 ***\n",
      "Epoch [3/5]  Batch [100/3125]  Loss: 0.002222\n",
      "Epoch [3/5]  Batch [200/3125]  Loss: 0.002221\n",
      "Epoch [3/5]  Batch [300/3125]  Loss: 0.002218\n",
      "Epoch [3/5]  Batch [400/3125]  Loss: 0.002239\n",
      "Epoch [3/5]  Batch [500/3125]  Loss: 0.002233\n",
      "Epoch [3/5]  Batch [600/3125]  Loss: 0.002203\n",
      "Epoch [3/5]  Batch [700/3125]  Loss: 0.002236\n",
      "Epoch [3/5]  Batch [800/3125]  Loss: 0.002194\n",
      "Epoch [3/5]  Batch [900/3125]  Loss: 0.002258\n",
      "Epoch [3/5]  Batch [1000/3125]  Loss: 0.002204\n",
      "Epoch [3/5]  Batch [1100/3125]  Loss: 0.002186\n",
      "Epoch [3/5]  Batch [1200/3125]  Loss: 0.002194\n",
      "Epoch [3/5]  Batch [1300/3125]  Loss: 0.002216\n",
      "Epoch [3/5]  Batch [1400/3125]  Loss: 0.002220\n",
      "Epoch [3/5]  Batch [1500/3125]  Loss: 0.002199\n",
      "Epoch [3/5]  Batch [1600/3125]  Loss: 0.002201\n",
      "Epoch [3/5]  Batch [1700/3125]  Loss: 0.002230\n",
      "Epoch [3/5]  Batch [1800/3125]  Loss: 0.002172\n",
      "Epoch [3/5]  Batch [1900/3125]  Loss: 0.002203\n",
      "Epoch [3/5]  Batch [2000/3125]  Loss: 0.002201\n",
      "Epoch [3/5]  Batch [2100/3125]  Loss: 0.002216\n",
      "Epoch [3/5]  Batch [2200/3125]  Loss: 0.002206\n",
      "Epoch [3/5]  Batch [2300/3125]  Loss: 0.002227\n",
      "Epoch [3/5]  Batch [2400/3125]  Loss: 0.002210\n",
      "Epoch [3/5]  Batch [2500/3125]  Loss: 0.002196\n",
      "Epoch [3/5]  Batch [2600/3125]  Loss: 0.002180\n",
      "Epoch [3/5]  Batch [2700/3125]  Loss: 0.002194\n",
      "Epoch [3/5]  Batch [2800/3125]  Loss: 0.002214\n",
      "Epoch [3/5]  Batch [2900/3125]  Loss: 0.002197\n",
      "Epoch [3/5]  Batch [3000/3125]  Loss: 0.002196\n",
      "Epoch [3/5]  Batch [3100/3125]  Loss: 0.002223\n",
      "*** Epoch 3 Complete.  Avg Loss = 0.002206 ***\n",
      "Epoch [4/5]  Batch [100/3125]  Loss: 0.002189\n",
      "Epoch [4/5]  Batch [200/3125]  Loss: 0.002197\n",
      "Epoch [4/5]  Batch [300/3125]  Loss: 0.002196\n",
      "Epoch [4/5]  Batch [400/3125]  Loss: 0.002208\n",
      "Epoch [4/5]  Batch [500/3125]  Loss: 0.002208\n",
      "Epoch [4/5]  Batch [600/3125]  Loss: 0.002178\n",
      "Epoch [4/5]  Batch [700/3125]  Loss: 0.002212\n",
      "Epoch [4/5]  Batch [800/3125]  Loss: 0.002171\n",
      "Epoch [4/5]  Batch [900/3125]  Loss: 0.002230\n",
      "Epoch [4/5]  Batch [1000/3125]  Loss: 0.002186\n",
      "Epoch [4/5]  Batch [1100/3125]  Loss: 0.002170\n",
      "Epoch [4/5]  Batch [1200/3125]  Loss: 0.002169\n",
      "Epoch [4/5]  Batch [1300/3125]  Loss: 0.002189\n",
      "Epoch [4/5]  Batch [1400/3125]  Loss: 0.002204\n",
      "Epoch [4/5]  Batch [1500/3125]  Loss: 0.002182\n",
      "Epoch [4/5]  Batch [1600/3125]  Loss: 0.002174\n",
      "Epoch [4/5]  Batch [1700/3125]  Loss: 0.002212\n",
      "Epoch [4/5]  Batch [1800/3125]  Loss: 0.002159\n",
      "Epoch [4/5]  Batch [1900/3125]  Loss: 0.002184\n",
      "Epoch [4/5]  Batch [2000/3125]  Loss: 0.002186\n",
      "Epoch [4/5]  Batch [2100/3125]  Loss: 0.002195\n",
      "Epoch [4/5]  Batch [2200/3125]  Loss: 0.002188\n",
      "Epoch [4/5]  Batch [2300/3125]  Loss: 0.002210\n",
      "Epoch [4/5]  Batch [2400/3125]  Loss: 0.002190\n",
      "Epoch [4/5]  Batch [2500/3125]  Loss: 0.002181\n",
      "Epoch [4/5]  Batch [2600/3125]  Loss: 0.002162\n",
      "Epoch [4/5]  Batch [2700/3125]  Loss: 0.002177\n",
      "Epoch [4/5]  Batch [2800/3125]  Loss: 0.002194\n",
      "Epoch [4/5]  Batch [2900/3125]  Loss: 0.002185\n",
      "Epoch [4/5]  Batch [3000/3125]  Loss: 0.002183\n",
      "Epoch [4/5]  Batch [3100/3125]  Loss: 0.002211\n",
      "*** Epoch 4 Complete.  Avg Loss = 0.002186 ***\n",
      "Epoch [5/5]  Batch [100/3125]  Loss: 0.002180\n",
      "Epoch [5/5]  Batch [200/3125]  Loss: 0.002182\n",
      "Epoch [5/5]  Batch [300/3125]  Loss: 0.002180\n",
      "Epoch [5/5]  Batch [400/3125]  Loss: 0.002193\n",
      "Epoch [5/5]  Batch [500/3125]  Loss: 0.002197\n",
      "Epoch [5/5]  Batch [600/3125]  Loss: 0.002165\n",
      "Epoch [5/5]  Batch [700/3125]  Loss: 0.002199\n",
      "Epoch [5/5]  Batch [800/3125]  Loss: 0.002161\n",
      "Epoch [5/5]  Batch [900/3125]  Loss: 0.002219\n",
      "Epoch [5/5]  Batch [1000/3125]  Loss: 0.002174\n",
      "Epoch [5/5]  Batch [1100/3125]  Loss: 0.002160\n",
      "Epoch [5/5]  Batch [1200/3125]  Loss: 0.002157\n",
      "Epoch [5/5]  Batch [1300/3125]  Loss: 0.002176\n",
      "Epoch [5/5]  Batch [1400/3125]  Loss: 0.002196\n",
      "Epoch [5/5]  Batch [1500/3125]  Loss: 0.002180\n",
      "Epoch [5/5]  Batch [1600/3125]  Loss: 0.002166\n",
      "Epoch [5/5]  Batch [1700/3125]  Loss: 0.002197\n",
      "Epoch [5/5]  Batch [1800/3125]  Loss: 0.002153\n",
      "Epoch [5/5]  Batch [1900/3125]  Loss: 0.002175\n",
      "Epoch [5/5]  Batch [2000/3125]  Loss: 0.002176\n",
      "Epoch [5/5]  Batch [2100/3125]  Loss: 0.002185\n",
      "Epoch [5/5]  Batch [2200/3125]  Loss: 0.002177\n",
      "Epoch [5/5]  Batch [2300/3125]  Loss: 0.002197\n",
      "Epoch [5/5]  Batch [2400/3125]  Loss: 0.002183\n",
      "Epoch [5/5]  Batch [2500/3125]  Loss: 0.002169\n",
      "Epoch [5/5]  Batch [2600/3125]  Loss: 0.002150\n",
      "Epoch [5/5]  Batch [2700/3125]  Loss: 0.002167\n",
      "Epoch [5/5]  Batch [2800/3125]  Loss: 0.002184\n",
      "Epoch [5/5]  Batch [2900/3125]  Loss: 0.002176\n",
      "Epoch [5/5]  Batch [3000/3125]  Loss: 0.002174\n",
      "Epoch [5/5]  Batch [3100/3125]  Loss: 0.002204\n",
      "*** Epoch 5 Complete.  Avg Loss = 0.002175 ***\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):\n",
    "        # input_seq: (batch, 15, 384), target_seq: (batch, 15, 384)\n",
    "        input_seq = input_seq.to(DEVICE)    # (BATCH_SIZE, SEQ_LEN−1,  EMBED_DIM)\n",
    "        target_seq = target_seq.to(DEVICE)  # (BATCH_SIZE, SEQ_LEN−1,  EMBED_DIM)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 6.6) Forward pass\n",
    "        output = model(input_seq, causal_mask)\n",
    "        # output shape: (batch, SEQ_LEN−1, EMBED_DIM)\n",
    "\n",
    "        # 6.7) Compute loss: MSE over all positions\n",
    "        # We do not want to predict beyond the provided target. Both output and target have shape (B, 15, 384).\n",
    "        # So we can do a straightforward MSE.\n",
    "        loss_tensor = criterion(output, target_seq)  # (B, 15, 384)\n",
    "        loss = loss_tensor.mean()                    # scalar\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}]  \"\n",
    "                f\"Batch [{batch_idx+1}/{len(train_loader)}]  \"\n",
    "                f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / n_batches\n",
    "    print(f\"*** Epoch {epoch} Complete.  Avg Loss = {avg_epoch_loss:.6f} ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23d412",
   "metadata": {},
   "source": [
    "Optionally, we can save the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d788eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL: bool = True\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    torch.save(model.state_dict(), \"saved_models/scm_v01.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e76a21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f791d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01b46ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from modules.prenet import PreNet\n",
    "from modules.encdec import get_gpt2_decoder\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "decoder, tokenizer = get_gpt2_decoder()\n",
    "\n",
    "prenet = PreNet(\n",
    "    input_dim=384,\n",
    "    output_dim=768,\n",
    "    bottleneck_dim=128,\n",
    "    prefix_len=20\n",
    ").to(DEVICE)\n",
    "\n",
    "prenet.load_state_dict(torch.load(\"saved_models/prenet_prefix_tuning_bookcorpus.pth\", map_location=DEVICE))\n",
    "\n",
    "\n",
    "def generative_inference(model, initial_sequence, n_future_steps):\n",
    "    model.eval()\n",
    "\n",
    "    prefix = initial_sequence.clone().unsqueeze(0).to(DEVICE)  # (1, k, input_dim)\n",
    "    generated = prefix.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_future_steps):\n",
    "            current_len = generated.size(1)\n",
    "            mask = generate_causal_mask(current_len, device=DEVICE)\n",
    "            out = model(generated, mask)\n",
    "            next_embed = out[:, -1, :].unsqueeze(1)\n",
    "            generated = torch.cat([generated, next_embed], dim=1)\n",
    "    return generated\n",
    "\n",
    "def vec_to_text(embedding, decoder, tokenizer, prenet, gen_len=50):\n",
    "    \"\"\"\n",
    "    Given input text, encode it, generate prefix via PreNet, and autoregressively decode output text.\n",
    "    \"\"\"\n",
    "    decoder.eval()\n",
    "    prenet.eval()\n",
    "    with torch.no_grad():\n",
    "        prefix = prenet(embedding.unsqueeze(0))  # (1, prefix_len, model_dim)\n",
    "\n",
    "        generated = prefix  # initial embeddings\n",
    "        generated_ids = []\n",
    "        for _ in range(gen_len):\n",
    "            outputs = decoder(inputs_embeds=generated)\n",
    "            next_logits = outputs.logits[:, -1, :]\n",
    "            next_id = torch.argmax(next_logits, dim=-1).unsqueeze(-1)  # greedy\n",
    "            generated_ids.append(next_id)\n",
    "            next_embed = decoder.transformer.wte(next_id)\n",
    "            generated = torch.cat([generated, next_embed], dim=1)\n",
    "\n",
    "    gen_ids = torch.cat(generated_ids, dim=1)\n",
    "    return tokenizer.decode(gen_ids[0].cpu().numpy(), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58a1c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_steps = 4\n",
    "\n",
    "sentences = [\n",
    "    \"``hello, my name is francesco, and this is my phd project''\",\n",
    "    \"he said, entering the univerisity class.\",\n",
    "]\n",
    "encoded_sentences = encoder.encode(sentences, convert_to_tensor=True)\n",
    "generated_seq = generative_inference(model, encoded_sentences, future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e544eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " , my name is diana . '' i say . '' mia , my name is diana . '' i say . '' mia , my\n",
      "---\n",
      " he said , entering the class . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "---\n",
      " , i 'm going to introduce you to the organization . '' '' '' i 'm going to introduce you to the organization . '' `` i '\n",
      "---\n",
      " , i 'm going to introduce you to the organization . '' '' '' i replied . '' i 'm going to introduce you to the organization .\n",
      "---\n",
      " , i 'm going to introduce you to the chairman of the institute . '' '' '' i replied . '' i 'm going to introduce you to\n",
      "---\n",
      " , i asked him to introduce you to the organization . '' '' i replied . '' i 'm a professor of mathematics . '' '' '' he replied\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for vec in generated_seq.squeeze():\n",
    "    generated_text = vec_to_text(vec, decoder, tokenizer, prenet, 30)\n",
    "    print(generated_text)\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
